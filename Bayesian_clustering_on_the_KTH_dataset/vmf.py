"""
This file includes functions to sample from von Mises-Fisher densities
and perform Maximum Likelihood Estimation of their parameters.
"""

import numpy as np
from scipy.optimize import minimize
from tqdm.auto import *
from Sampler_class import *

import stiefel as st
import spa


def log_von_mises_fisher(Y, F, normalized=False):
    """von Mises-Fisher log-density on the Stiefel Manifold"""
    if normalized:
        return (Y*F).sum() - spa.log_vmf(F)
    else:
        return (Y*F).sum()

def sample_von_mises_fisher_slice(F, n_iter=100, burn=100, stride=10):
    """
    Sample from the vMF distribution using an adaptive Metropolis-Hastings algorithm.
    """
    X_0 = st.proj_V(F) # Initialize at the mode of the distribution to ensure a fast convergence.
    n, p = F.shape
    
    total_steps = burn+n_iter*stride
    
    Ind_sel=[ i for i in range(burn,total_steps,stride)]
    log_prob = lambda x : np.einsum('ik,ik->k',F,x).sum()
    sampler_slice=Sampler_Slice_Sampling_Geodesics(log_prob,n,p)
    data = sampler_slice.run_kernel(X_0, total_steps,w=7,use_adapt=False,m=1)
    Xs=np.zeros((len(Ind_sel),n,p))
    for k,i in enumerate(Ind_sel):
        Xs[k]=data[i]
    return Xs      

def sample_von_mises_fisher(F, n_iter=100, burn=100, stride=10, progress=False):
    """
    Sample from the vMF distribution using an adaptive Metropolis-Hastings algorithm.
    """
    X = st.proj_V(F) # Initialize at the mode of the distribution to ensure a fast convergence.
    s = np.linalg.svd(F, compute_uv=False)
    n, p = F.shape
    Xs = np.zeros((n_iter, n, p))
    current_lk = (X*F).sum()
    accepts = 0
    
    # Adaptive parameters: the proposal variance is tuned along the MCMC
    std = 0.4
    batch = 100
    accepts_hist = np.zeros(batch)
    optimal_rate = 0.234
    
    total_steps = burn+n_iter*stride
    it = trange(total_steps) if progress else range(total_steps)
    for t in it:
        # The proposal is generated by adding non-manifold noise and projecting back onto the manifold.
        D = std * np.random.randn(n, p)/s
        X2 = st.proj_V(X + D)
        new_lk = (X2*F).sum()
        
        if new_lk - current_lk > np.log(np.random.rand()):
            X = X2
            current_lk = new_lk
            accepts += 1
            accepts_hist[t%batch] = 1
        else:
            accepts_hist[t%batch] = 0
        
        if t >= burn and (t-burn)%stride==0:
            Xs[(t-burn)//stride] = X
        
        if t%batch==0 and t>0:
            adapt = 2*(accepts_hist.mean() > optimal_rate) - 1
            std = np.exp(np.log(std) + adapt/np.sqrt(t))
            
    if progress: print(f"VMF Acceptance rate: {accepts/(burn+n_iter*stride)}")
    return Xs


def mle(X_bar, orth=False, upper_sv=500):
    """Maximum Likelihood Estimate of samples of a vMF distribution from their Euclidean mean X_bar."""
    
    n, p = X_bar.shape
    
    u, g, v = np.linalg.svd(X_bar, full_matrices=False)
    
    if orth: # if True, constrains the columns of the MLE F to be orthogonal
        u = u@v
        v = np.eye(p)

    def mle_objective(s):
        F = u@np.diag(s)@v
        C = spa.log_vmf(F,s,diag=True)
        res = (X_bar*F).sum() - C
        return -res
    
    res = minimize(mle_objective, x0=g, tol=1e-2, bounds=[(1e-2,upper_sv)]*p)
    s = res['x']
    
    return u@np.diag(s)@v